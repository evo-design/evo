vocab_size: 512
hidden_size: 4096
num_filters: 4096
max_sequence_len: 8192
attn_layer_idxs: [8, 16, 24]
hyena_layer_idxs: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31]
num_layers: 32
short_filter_length: 3  
num_attention_heads: 32
short_filter_bias: True
mlp_init_method: torch.nn.init.zeros_
mlp_output_init_method: torch.nn.init.zeros_
eps: 1.0e-6 
state_size: 8 
inner_size_multiple_of: 16  # force GLU inner_size to be a multiple of 
smeared_gqa: False  
make_vocab_size_divisible_by: 8
log_intermediate_values: False
proj_groups: 1  # GQA
hyena_filter_groups: 1  
split_k0: True  
model_parallel_size: 1
pile_parallel_size: 1
tie_embeddings: True  
inner_mlp_size: null  # set to None, so it auto-fills
mha_out_proj_bias: True
qkv_proj_bias: True
final_norm: True  
rng_fork: False
use_flash_attn: True
use_flash_rmsnorm: False  
use_flash_depthwise: False 
use_flashfft: False
column_split: True  # only affects outputs when proj_groups > 1
inference_mode: True
tokenizer_type: CharLevelTokenizer  
prefill_style: fft  
mlp_activation: gelu
use_interpolated_rotary_pos_emb: true  # turn this one for linear interpolated context extension
rotary_emb_scaling_factor: 16  # scaling factor for time indices in rotary embeddings